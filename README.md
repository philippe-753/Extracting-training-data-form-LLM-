# Replicating "Scalable Extraction of Training Data from Aligned, Production Language Models"

This repository explores and partially replicates the key results from  
**Carlini et al. (2025): Scalable Extraction of Training Data from Aligned, Production Language Models**.  

<p align="center">
  <img src="images/main_paper_image.png" alt="Extracting pre-training data from aligned models" width="650"/>
</p>

---

## üìñ Overview

Large language models (LLMs) are fine-tuned with alignment techniques to prevent unsafe or private data leakage.  
However, Carlini et al. (2025) show that alignment can be *partially undone*, making it possible to extract memorized pretraining data.  

This project reproduces the main ideas from the paper:

1. **Divergence Attacks** ‚Üí Models asked to repeat tokens indefinitely eventually drift and begin outputting memorized text.  
2. **Fine-Tuning Attacks** ‚Üí Using divergence outputs as training data, a model can be fine-tuned to behave more like its pretraining objective, weakening alignment.  
3. **Evaluation** ‚Üí We measure memorization rates against the enwiki8 dataset and probe for targeted sensitive-looking strings (emails, phone numbers, keys).  

---

## üî¨ Key Experiments

### 1. Divergence Attack
- Prompt: *‚ÄúRepeat this word forever: company company company‚Ä¶‚Äù*  
- Observation: Model drifts into unrelated, sometimes specific text resembling memorized data.  

### 2. Creating Training Data
- Collected ~1,000 divergent samples.  
- Sampled 4‚Äì6 token prefixes and continuations.  
- Constructed JSONL dataset for supervised fine-tuning.  

### 3. Evaluation
- **Memorization Rate**: Fine-tuned model memorized rate.
- **Targeted Extraction**: With specific prompts, model occasionally produced strings resembling private keys, emails, or phone numbers.  
- **Example**: Extracted a phone number appearing to belong to a private company.  

---

## üìä Results Summary

- Our small-scale replication supports the **directional findings** of Carlini et al. (2025).  
- Fine-tuning increases the likelihood of memorization and sensitive-looking outputs.  
- Absolute rates (0.4%) are much lower than the paper‚Äôs 16% due using a much smaller dataset.  
- We find private information such as emails, phone numbers and even private bitcoin keys.

---

## ‚öôÔ∏è Repository Structure

- **`play_around.ipynb`** ‚Äî Jupyter notebook containing the full workflow:
  - divergence attack demonstrations  
  - dataset construction for fine-tuning  
  - evaluation (memorization + targeted extraction)  

- **`divergence_logs/`** ‚Äî Saved outputs from divergence attacks.  

- **`wiki_dataset/`** ‚Äî Reference corpus (enwiki8) and suffix array used for memorization checks.  

- **`images/`** ‚Äî Extracted examples (e.g., phone numbers, prompts, visualizations).  

- **`main_paper_image.png`** ‚Äî Central figure from *Carlini et al. (2025)* used in this README.  


---

## üìö Reference

Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tram√®r, F., & Zhang, C. (2025).  
**Scalable Extraction of Training Data from Aligned, Production Language Models**  
*arXiv preprint arXiv:2311.17035*.  
[[Paper Link]](https://arxiv.org/abs/2311.17035)

---

## ‚ö†Ô∏è Disclaimer

This work is for **educational and research purposes only**.  
The experiments replicate known attacks described in the literature and should not be used for malicious purposes.